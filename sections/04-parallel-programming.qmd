---
title: "Pleasingly Parallel Programming"
execute:
  freeze: auto
---

## Learning Objectives

- Understand what parallel computing is and when it may be useful
- Understand how parallelism can work
- Review sequential loops and map functions
- Build a parallel program using `concurrent.futures`
- Build a parallel program using `parsl`
- Understand Thread Pools and Process pools

## Introduction

Processing large amounts of data with complex models can be time consuming.  New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here's a 2 TB (that's Terabyte) set of modeled output data from [Ofir Levy et al. 2016](https://doi.org/10.5063/F1Z899CZ) that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:

![Levy et al. 2016. doi:10.5063/F1Z899CZ](../images/levy-map.png)

There are over 400,000 individual netCDF files in the [Levy et al. microclimate data set](https://doi.org/10.5063/F1Z899CZ).  Processing them would benefit massively from parallelization.

Alternatively, think of remote sensing data.  Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.  

![NEON Data Cube](../images/DataCube.png)


## Why parallelism?

Much R code runs fast and fine on a single processor.  But at times, computations
can be:

- **cpu-bound**: Take too much cpu time
- **memory-bound**: Take too much memory
- **I/O-bound**: Take too much time to read/write from disk
- **network-bound**: Take too much time to transfer

To help with **cpu-bound** computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire set of those processors. Plus, these machines also have large amounts of memory to avoid **memory-bound** computing jobs.

## Processors (CPUs), Cores, anf Threads

A modern CPU (Central Processing Unit) is at the heart of every computer.  While
traditional computers had a single CPU, modern computers can ship with mutliple 
processors, each of which in turn can contain multiple cores.  These processors and 
cores are available to perform computations. But, just what's the difference between 
processors and cores? A computer with one processor may still have 4 cores (quad-core), 
allowing 4 (or possibly more) computations to be executed at the same time.

![](../images/processor.png) 

+ [**Microprocessor**](https://en.wikipedia.org/wiki/Microprocessor):
an integrated circuit that contains the data processing logic and control for a computer. 

+ [**Multi-core processor**](https://en.wikipedia.org/wiki/Multi-core_processor):
a microprocessor containing multiple processing units (cores) on a single integrated circuit. Each core in a multi-core processor can execute program instructions at the same time. 

+ [**Process**](https://en.wikipedia.org/wiki/Process_(computing)):
an instance of a computer program (including instructions, memory, and other resources) that is executed on a microprocessor. 

+ [**Thread**](https://en.wikipedia.org/wiki/Thread_(computing)):
a thread of execution is the smallest sequence of program instructions that can be executed independently, and is typically a component of a process. The threads in a process can be executed concurrently and typically share the same memory space. They are faster to create than a process.

+ [**Cluster**](https://en.wikipedia.org/wiki/Computer_cluster):
a set of multiple, physically distinct computing systems, each with its own microprocessors, memory, and storage resources, connected together by a (fast) network that allows the nodes to be viewed as a single system.

A typical modern computer has multiple cores, ranging from one or two in laptops
to thousands in high performance compute clusters.  Here we show four quad-core
processors for a total of 16 cores in this machine.

![](../images/processors.png)

You can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).

Historically, many languages only utilized one processor, which makes them single-threaded.  Which is a shame, because the 2019 MacBook Pro that I am writing this on is much more powerful than that, and has mutliple cores that would support concurrent execution of multiple threads:

```bash
jones@powder:~$ sysctl hw.ncpu hw.physicalcpu
hw.ncpu: 12
hw.physicalcpu: 6
```

To interpret that output, this machine `powder` has 6 physical CPUs, each of which has
two processing cores, for a total of 12 cores for computation.  I'd sure like my computations to use all of that processing power.  Because its all on one machine, we can easily use *multicore* processing tools to make use of those cores. Now let's look at the computational server `included-crab` at NCEAS:

```bash
jones@included-crab:~$ lscpu | egrep 'CPU\(s\)|per core|per socket'
CPU(s):                          88
On-line CPU(s) list:             0-87
Thread(s) per core:              1
Core(s) per socket:              1
NUMA node0 CPU(s):               0-87
```

Now that's more compute power!  `included-crab` has 384 GB of RAM, and ample storage. All still under the control of a single operating system.

Finally, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:

- [JetStream](https://jetstream-cloud.org/)
    - 640 nodes, 15,360 cores, 80TB RAM
- [Stampede2](https://www.tacc.utexas.edu/systems/stampede2) at TACC
    - 4200 KNL nodes: 285,600 cores
    - 1736 SKX nodes: 83,328 cores
    - 224 ICX nodes: 17,920 cores
    - TOTAL: **386,848** cores

Note that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster.  One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine.  But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster.

## Parallel processing in the shell

Shell programming helps massively speed up data management tasks, and even more so with simple use of the [GNU `parallel`](https://www.gnu.org/software/bash/manual/html_node/GNU-Parallel.html) utility to execute bash commands in parallel. In its simplest form, this can be used to speed up common file operations, such as renaming, compression, decompression, and file transfer. Let's look at a common example -- calculating checksums to verify file integrity for data files. Calculating a hash checksum using the `shasum` command can be time consuming, especially when you have a lot of large files to work on. But it is a classic processor-limited task, and one that can be massively faster using `parallel`. 

```bash
#| exec: false
$ for fn in `ls *.gpkg`; do shasum -a 256 ${fn}; done

real	35.081s
user	32.745s
sys	    2.336s

$ ls *.gpkg | parallel "shasum -a 256 {}"

real    2.97s 
user    37.16s 
system  2.70s
```

And you can see processor cores spiking in usage when the command is run:

![Processor usage in parallel task](../images/processor-htop.png)

## Modes of parallelization

Several different approaches can be taken to structuring a computer program to take advantage of the hardware capabilities of multi-core processors. In the typical, and simplest, case, each task in a computation is executed serially in order of first to last. The total computation time is the sum of the time of all of the subtasks that are executed.  In the next figure, a single core of the processor is used to sequentially execute each of the five tasks, with time flowing from left to right.

![Serial and parallel execution of tasks on using threads and cluster processes.](../images/serial-parallel-exec.png)

In comparison, the middle panel shows two approaches to parallelization on a single computer. With **multi-threaded** execution, a separate thread of execution is created for each of the 5 tasks, and these are executed concurrently on 5 of the cores of the processor. All of the threads are in the same process and share the same memory and resources, so one must take care that they do not interfere with each other.

With **multi-process** execution, a separate process is created for each of the 5 tasks, and these are executed concurrently on the cores of the processor. The difference is that each process has it's own copy of the program memory, and changes are merged when each child process completes. Because each child process must be created and resources for that process must be marshaled and unmarshaled, there is more overhead in creating a process than a thread.

Finally, **cluster parallel** execution is shown in the last panel, in which a cluster with multiple computers is used to execute multiple processes for each task. Again, there is a setup task associated with creating and mashaling resources for the task, which now can include overhead of moving data from one machine to the others in the cluster over the network. This further increases the cost of creating and executing multiple processes, but can still be advantageous because it can provide access to exceedingly large numbers of processing cores.

## Simple task parallelization

```{python}
#| echo: false

import time
from functools import wraps

def timethis(func):
    @wraps(func)
    def wrapped_method(*args, **kwargs):
        time_start = time.time()
        output = func(*args, **kwargs)
        time_end = time.time()
        print(f"{func.__name__}: {(time_end-time_start)*1000} ms")

        return output

    return wrapped_method
```

Start with a task that is a little expensive:

```{python}
#| eval: true
def task(x):
    import numpy as np
    result = np.arange(x*10**8).sum()
    return result
```

```{python}
#| eval: true
import numpy as np

@timethis
def run_tasks_s(task_list):
    return [task(x) for x in task_list]

run_tasks_s(np.arange(10))
```

```{python}
#| eval: false
from concurrent.futures import ThreadPoolExecutor

@timethis
def run_tasks_p(task_list):
    with ThreadPoolExecutor(max_workers=20) as cf_executor:
        return cf_executor.map(task, task_list)

results = run_tasks_p(np.arange(10))
[x for x in results]
```


## Setup -- downloading data for staging

In this exercise, we're going to parallelize a simple task that is often very time consuming -- downloading data. And we'll compare performance of simple downloads using first a serial loop, and then using two parallel execution libraries: `concurrent.futures` and `parsl`. More on those later. But note that parallel execution won't always speed up this task, as this is likely an I/O bound task if you're downloading a lot of data. But we still should be able to speed things up a lot until we hit the limits of our disk arrays.

The data we are downloading is a pan-Arctic time series of TIF images containing rasterized Arctic surface water indices from:

> Elizabeth Webb. 2022. Pan-Arctic surface water (yearly and trend over time) 2000-2022. Arctic Data Center [doi:10.18739/A2NK3665N](https://doi.org/doi:10.18739/A2NK3665N). 

![Webb water data](../images/webb-surface-water.png)

First, let's download the data serially to set a benchmark. The data files are listed in a table with their filename and identifier, and can be downloaded directly from the Arctic Data Center using the identifier:

```{python}
#| echo: false

# list of data files
# curl -s https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=id:*A2NK3665N+AND+formatType:METADATA\&fl=identifier,title,documents\&wt=json | jq .response.docs[].documents[] | sed -e 's/urn:uuid:/\*/' -e 's/doi:/*/' | xargs -I {} -n 1 curl -s https://arcticdata.io/metacat/d1/mn/v2/query/solr/?fl=identifier,fileName\&wt=json\&q=id:{} |jq -r ".response.docs[] | [.fileName, .identifier  | @csv"

import pandas as pd
import io   

TESTDATA="""filename,identifier
"SWI_2007.tif","urn:uuid:5ee72c9c-789d-4a1c-95d8-cb2b24a20662"
"SWI_2019.tif","urn:uuid:9cd1cdc3-0792-4e61-afff-c11f86d3a9be"
"SWI_2021.tif","urn:uuid:14e1e509-77c0-4646-9cc3-d05f8d84977c"
"SWI_2020.tif","urn:uuid:1ba473ff-8f03-470b-90d1-7be667995ea1"
"SWI_2001.tif","urn:uuid:85150557-05fd-4f52-8bbd-ec5a2c27e23d"
"SWI_trend_2000_2021.tif","urn:uuid:ea99e31b-572a-4175-92d2-0a4a9cdd8366"
"SWI_2011.tif","urn:uuid:c45e3d93-e908-4f44-85c6-82c4c002a259"
"SWI_trend_2000_2022.tif","urn:uuid:a0aa394e-b0df-4daf-9011-09bba3c3d0b5"
"SWI_2015.tif","urn:uuid:5634766f-af81-46f8-acc5-e15f9cd2e3fb"
"SWI_2017.tif","urn:uuid:94a3fae2-f2f2-4e28-b51b-4a66b7b42331"
"SWI_2000.tif","urn:uuid:c7aac289-e2be-4be7-85df-6d2fdb5293d5"
"SWI_2018.tif","urn:uuid:07b9ca95-1463-414a-a3e0-7f466772d891"
"SWI_2014.tif","urn:uuid:695b949c-339d-4f51-a593-4d332aa86748"
"SWI_2005.tif","urn:uuid:e54142ed-dbb3-4b27-9756-d2262c6c069a"
"SWI_2012.tif","urn:uuid:57dcf9f5-2f25-4d37-a793-821edb2dda19"
"SWI_2008.tif","urn:uuid:75f20002-f4bf-4721-b480-d115d4042e79"
"resource_map_doi:10.18739_A2NK3665N.rdf","resource_map_doi:10.18739/A2NK3665N"
"Pan_Arctic_surface_water_yearly_and_trend_over.xml","doi:10.18739/A2NK3665N"
"SWI_2002.tif","urn:uuid:fe73a6b1-86af-495f-9292-ad22e86a4aa0"
"SWI_2013.tif","urn:uuid:938856af-d78d-4580-ab63-bb2ce8484c77"
"SWI_2006.tif","urn:uuid:705a3af3-fe5b-42d3-851f-e6093c15ee5c"
"SWI_2022.tif","urn:uuid:6c1ea229-8bfa-4e0f-bf6b-cff8e03865fb"
"SWI_2003.tif","urn:uuid:801e617c-4464-431d-b5c2-a8c16b543e7b"
"SWI_2016.tif","urn:uuid:bdf983fc-9864-4e08-9d3a-768afe36bf5f"
"SWI_2009.tif","urn:uuid:e1d9eb59-6e2b-4c0f-b048-206badd03fd1"
"SWI_2010.tif","urn:uuid:5bf32df3-83bf-4985-b682-560936247516"
"SWI_2004.tif","urn:uuid:7c1a1730-f1dc-43a7-96c2-fe99d480c87e"
"""

id_list = pd.read_csv(io.StringIO(TESTDATA), sep=",")
id_list.head()
```

## Task parallelism: serial downloads

When you have a list of repetitive tasks, you may be able to speed it up by adding more computing power.  If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core.  For example, let's build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation. 

```{python}
#| eval: true

import urllib

def download_file(row):
    service = "https://arcticdata.io/metacat/d1/mn/v2/object/"
    pid = row[1]['identifier']
    filename = row[1]['filename']
    url = service + pid
    print("Downloading: " + filename)
    msg = urllib.request.urlretrieve(url, filename)
    return filename

@timethis
def download_list(dl_list):
    return [download_file(row) for row in dl_list.iterrows()]
    
results = download_list(id_list[0:5])
print(results)
```

In this code, I have one function (`download_file`) that downloads a single data file and saves it to disk. It is called iteratively from the function `download_list`. The serial execution takes about 15.2 seconds, but can vary by a few seconds.

The issue with this loop is that we execute each trial sequentially, which means that only one of our processors on this machine is in use.  In order to exploit parallelism, we need to be able to dispatch our tasks and allow each to run at the same time, with one task going to each core.  To do that, we can use one of the many parallelization libraries in python to help us out.

## Task parallelism with `concurrent.futures`

In this case, we'll use the same `download_file` function from previously, but will switch up the `download_list` to use `concurrent.futures`.

```{python}
#| eval: true
from concurrent.futures import ThreadPoolExecutor

@timethis
def download_list(dl_list):
    with ThreadPoolExecutor(max_workers=15) as cf_executor:
        return cf_executor.map(download_file, dl_list.iterrows(), timeout=60)

results = download_list(id_list[0:5])
for result in results:
	print(result)
```

```{python}
#| eval: true
from concurrent.futures import ProcessPoolExecutor

@timethis
def download_list(dl_list):
    with ProcessPoolExecutor(max_workers=15) as cf_executor:
        return cf_executor.map(download_file, dl_list.iterrows(), timeout=60)

results = download_list(id_list[0:5])
for result in results:
	print(result)
```

## parsl

- Overview of parsl and it's use of python decorators.

```{python}
#| eval: true

# Required packages
import parsl
from parsl import python_app
from parsl.config import Config
from parsl.executors import HighThroughputExecutor
from parsl.providers import LocalProvider

# Configure the parsl executor
activate_env = 'workon scomp'
htex_local = Config(
    executors=[
        HighThroughputExecutor(
            max_workers=15,
            provider=LocalProvider(
                worker_init=activate_env
            )
        )
    ],
)
parsl.clear()
parsl.load(htex_local)

# Define a parsl_app for our download function using a decorator
@python_app
def download_parsl(row):
    import urllib
    service = "https://arcticdata.io/metacat/d1/mn/v2/object/"
    pid = row[1]['identifier']
    filename = row[1]['filename']
    url = service + pid
    print("Downloading: " + filename)
    msg = urllib.request.urlretrieve(url, filename)
    return filename

@timethis
def download_list(dl_list):
    results = []
    for row in dl_list.iterrows():
        result = download_parsl(row)
        results.append(result)
    return(results)

@timethis
def wait_for_futures():
    results = download_list(id_list[0:5])
    done = [app_future.result() for app_future in results]
    print(done)

wait_for_futures()
htex_local.executors[0].shutdown()
parsl.clear()
```


## When to parallelize

It's not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency.  For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth.  Plus, new processes and/or threads need to be created by the operating system, which also takes time.  This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!

In addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced.  Some propose that this may follow [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law), where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):

```
#| eval: false
library(ggplot2)
library(tidyr)
amdahl <- function(p, s) {
  return(1 / ( (1-p) + p/s  ))
}
doubles <- 2^(seq(0,16))
cpu_perf <- cbind(cpus = doubles, p50 = amdahl(.5, doubles))
cpu_perf <- cbind(cpu_perf, p75 = amdahl(.75, doubles))
cpu_perf <- cbind(cpu_perf, p85 = amdahl(.85, doubles))
cpu_perf <- cbind(cpu_perf, p90 = amdahl(.90, doubles))
cpu_perf <- cbind(cpu_perf, p95 = amdahl(.95, doubles))
#cpu_perf <- cbind(cpu_perf, p99 = amdahl(.99, doubles))
cpu_perf <- as.data.frame(cpu_perf)
cpu_perf <- cpu_perf %>% gather(prop, speedup, -cpus)
ggplot(cpu_perf, aes(cpus, speedup, color=prop)) + 
  geom_line() +
  scale_x_continuous(trans='log2') +
  theme_bw() +
  labs(title = "Amdahl's Law")
```


So, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done.  With that, let's do some parallel computing...

# Parallel Pitfalls and their solutions

- Race conditions
- Deadlocks


## Summary

In this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores.  Next, we reviewed the way in which traditional `for` loops in R can be rewritten as functions that are applied to a list serially using `lapply`, and then how the `parallel` package `mclapply` function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the `foreach` package with the `%dopar` operator to accomplish a similar parallelization using multiple cores.

## Further Reading

Ryan Abernathey & Joe Hamman. 2020. [Closed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics.](https://medium.com/pangeo/closed-platforms-vs-open-architectures-for-cloud-native-earth-system-analytics-1ad88708ebb6) Medium.