---
title: "Parallelization with Dask"
---
## Introduction
Dask is a library for parallel computing in Python. 
It can scale up code to use the full capacity of your personal computer or to distribute work in a cloud cluster. 
By mirroring APIs of other commonly used Python libraries such as Pandas and NumPy, Dask provides a familiar interface that makes it easier to parallelize your code. 
In this lesson, we will get acquainted with some of Dask's most commonly used objects and Dask's way of distributing and evaluating computations. 

![ ](../images/dask_logo.png)

## Objectives

- Become familiar with Dask processing workflow:
    - What are the client, scheduler, workers, and cluster
    - Understand delayed computations and "lazy" evaluation
    - Obtain information about computations via the Dask dashboard
    
- Learn basics of `dask.arrays` and `dask.dataframes`:
    - Load data and specify partition/chunk sizes
    - Interpret a task graph

- Integrate `xarray` with Dask for geospatial computations
    
- Share best practices and resources for further reading

## Dask Cluster
We can deploy a Dask cluster on a single machine or an actual cluster with multiple machines. 
We chose to use the Dask cluster in this lesson instead of the default Dask scheduler to take advantage of the *cluster dashboard*, which keeps track of the performance and progress of our computations. 

Dask clusters have three main components for processing computations in parallel. These are the *client*, the *scheduler* and the *workers*.

- When we code, we communicate directly with the **client**, which is responsible for submitting tasks to be executed to the scheduler.

- After receiving the tasks from the client, the **scheduler** determines how tasks will be distributed among the workers and coordinates them to process tasks in parallel.

- Finally, the **workers** are threads, processes, or separate machines in a cluster. They compute tasks and store and return computations results.

In order to interact with the client and generate tasks that can be processed in parallel we need to use Dasks' objects to read our data. Here we will see examples of how to use `dask.dataframes` and `dask.arrays`.

![From: https://www.datarevenue.com/en-blog/understanding-dask-architecture-client-scheduler-workers Make this into a collored graph, add cluster envelope ](../images/dask_cluster.png)


### Setting up a Local Cluster
We can create a local cluster as follows:

```{python}
from dask.distributed import LocalCluster, Client
```

```{python}
#| eval: false
cluster = LocalCluster(n_workers=5, memory_limit=0.1, processes=False)
cluster
```

![](../images/dask_cluster_panel.png)

And then we create a client to connect to our cluster:

```{python}
#| eval: false
client = Client(cluster)
client
```
![](../images/dask_client.png)

Learn more about:
    - [Dask clusters](https://docs.dask.org/en/latest/deploying.html)
    - [Threads and processes](https://towardsdatascience.com/which-is-faster-python-threads-or-processes-some-insightful-examples-26074e90848f)


### Dask Dashboard
When we set up a cluster we can see the **Dask dashboard** address by looking at either the client or the cluster. 
This dashboard offers live monitoring of all the Dask computations. 
The dashboard's main page shows diagnostics about:

    - the cluster's and individual worker's memory usage
    - number of tasks being processed by each worker
    - individual tasks being processed across workers
    - progress towards completion of each individual task

There's a lot to talk about how to interpret the diagnostics in the Dask dashboard. We recommend this part of the documentation to understand the [basics of the dashboard diagnostics](https://docs.dask.org/en/latest/dashboard.html#dashboard-memory)
, and this video as a [deeper dive on the dashboard's functions.

![View of Dask dashboard](../images/dask_dashboard.png)


## `dask.dataframes`
When we analyze tabular data, we usually start our analysis by loading it into memory as a Pandas DataFrame. 
But, what if this data does not fit in memory? 
Or maybe our analyzes crash because we run out of memory. 
These scenarios are typical entry points into parallel computing. 
In such cases, Dask’s scalable alternative to a Pandas DataFrame is the `dask.dataframe`. 
A `dask.dataframe` is made up of many `pd.DataFrames`, each containing a subset of rows of the original dataset.
We call each of these pandas pieces a **partition** of the `dask.dataframe`.


![ Dask Array design ([dask documentation](https://docs.dask.org/en/latest/dataframe.html))](../images/dask_dataframe.png)


### Reading a csv
To get familiar with `dask.dataframes` we will use tabular data of soil mositure measurements at six forest stands in northeastern Siberia. 
The data has been collected since 2014 and is archived at the Arctic Data Center ([Loranty & Alexander, doi:10.18739/A24B2X59C](https://arcticdata.io/catalog/view/doi%3A10.18739%2FA24B2X59C)).
Just as we did on the previous lesson, we download the data using the `requests` package and the data's url obtained from the Arctic Data Center. 

```{python}
import os              
import requests 
```

```{python}
#| eval: false
url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A27e4043d-75eb-4c4f-9427-0d442526c154'

response = requests.get(url)
open("dg_soil_moisture.csv", "wb").write(response.content)

```

In the Arctic Data Center metadata we can see this file is 115MB. 
To import this file as `a dask.dataframe` with more than one partition we need to specify the size of each partition with the `blocksize` parameter. 
In this example we will split the data frame into six partitions, which would mean a blocksize of approximately 20 MB.

:::{.callout-note collapse=True}
*About the encoding parameter:*  
If we try to import the file directly we will obtain the error `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb3 in position 192:` `invalid start byte`. 
This means XXXX.
To find the encoding of the file and add the appropiate encodign to `dask.dataframe.read_csv` we can run the following code.
```{python}
#| eval: false
import chardet
fp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')
with open(fp, 'rb') as rawdata:
    result = chardet.detect(rawdata.read(100000))
result

```
:::

```{python}
import dask.dataframe as dd

fp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')
df = dd.read_csv(fp, blocksize = '20MB' , encoding='ISO-8859-1')
```

Notice that we cannot see any values in the data frame. 
This is because Dask has not really loaded the data, it will wait until we explicitely ask it to print or compute something to do so. 
However, we can still do `df.head()`, it's not costly for memory to just access a few rows of the data frame. 

```{python}
df.head(2)
```

### Lazy Computations  
The application programming interface (API) of a `dask.dataframe` is a subset of the `pd.DataFrame` API.
So if you are familiar with pandas, many of the core `pandas.DataFrame` methods directly translate to `dask.dataframes`. 

```{python}
# an example of a simple df transformation
```

Notice that XXXX.
A major difference between  `pandas.DataFrames` and `dask.dataframes`and  is that `dask.dataframes` are *lazy*.
This means that an object will queue transformations and calculations without executing them, up until we explicitly ask for the result of that chain of computations using the `compute` method.
Once we run `compute`, the scheduler can allocate memory and workers to execute the computations in parallel.
This kind of **lazy computations** is how most Dask workloads work.
This varies from **eager evaluation** methods and functions, which start computing results right when they are executed.

### Task Graph

Lead into task graphs: XXX
```{python}
### THIS IS AN EXAMPLE OF TASK GRAPH

##!! cannot run graph after installing graphviz with
#pip install graphviz

# Error:
# ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH

# See:
#https://stackoverflow.com/questions/42014458/dask-not-installing-graphviz-dependency
```


## `dask.arrays`
Another common object we might want to parallelize is a NumPy array. 
The equivalent Dask object is the `dask.array`, which coordinates many NumPy arrays that may live on disk or other machines. 
Each of these NumPy arrays within the `dask.array` is called a **chunk**. 
Choosing how these chunks are arranged within the `dask.array` and their size can significantly affect the performance of our code. 
Here you can find more information about [chunks](https://docs.dask.org/en/latest/array-chunks.html) and [best practices](https://docs.dask.org/en/latest/array-best-practices.html).


![ Dask Array design ([dask documentation](https://docs.dask.org/en/latest/array.html)](../images/dask_array.png)


```{python}
import dask.array as da
import numpy as np
```


```{python}
data = np.arange(100_000).reshape(200, 500)
a = da.from_array(data, chunks=(100, 100))
```

Indexing Dask collections feels just like slicing NumPy arrays:

```{python}
#| eval: false
a[:2, 2]
```

And computations also work lazily:

```{python}
#| eval: false
a.mean()
```

```{python}
#| eval: false
a.mean().compute()
```

## Dask and xarray
Going forward, it might be more common having to read some big array-like dataset (like a high-resolution multiband raster) than creating one from scratch from a NumPy array. 
In this case it can be useful to use the `xarray` module together with Dask. 
It is simple to wrap Dask around `xarray` objetcs, we only need to specify the number of chunks argument as an argument when we are reading in a dataset (see also [[1]](https://docs.xarray.dev/en/stable/user-guide/dask.html)).

### Open .tif file
As an example, let's do an NDVI calculation using remote sensing imagery collected by aerial vehicles over northeastern Siberia (Loranty, Forbath, Talucci, Alexander, DeMarco, et al. 2020. Uncrewed aerial vehicle remote sensing imagery of postfire vegetation in Siberian larch forests 2018-2019. [Arctic Data Center. doi:10.18739/A2ZC7RV6H](https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2ZC7RV6H).). 

First we download the data for the near-infrared (NIR) and red bands:

```{python}
#| eval: false
# download red band
url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Aac25a399-b174-41c1-b6d3-09974b161e5a'
response = requests.get(url)
open("RU_ANS_TR2_FL005M_red.tif", "wb").write(response.content)

# download nir band
url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A1762205e-c505-450d-90ed-d4f3e4c302a7'
response = requests.get(url)
open("RU_ANS_TR2_FL005M_nir.tif", "wb").write(response.content)
```


Because these are .tif files and have geospatial metadata, we will use `xarray.open_rasterio` to read them.
We need to specify either a shape or the size in bytes for each chunk to indicate we will open these files with `dask.arrays` as the underlying object to the `xarray.DataArray` (instead of a `numpy.array`). 
Both files are 76 MB, so let's have chunks of 15 MB to have roughly six chunks.


```{python}
import xarray as xr
```

```{python}
# read in the file
fp_red = os.path.join(os.getcwd(),"RU_ANS_TR2_FL005M_red.tif")
red = xr.open_rasterio(fp_red, chunks = '15MB')
```

We can see a lot of useful information here:

- There are eight chunks in the array. We were aiming for six, but this often happens with the way Dask distributes the memory (76MB is not divisible by 6).
- There is geospatial information (transformation, CRS, resolution) and no-data values.
- There is an unnecessary dimension: a constant value for the band. So our next step is to squeeze the array to flatten it.

```{python}
# getting rid of unnecessary dimension
red = red.squeeze()
```

Next we read in the NIR band and do the same pre-processing. 

```{python}
fp_nir = os.path.join(os.getcwd(),"RU_ANS_TR2_FL005M_nir.tif")
nir = xr.open_rasterio(fp_nir, chunks = '15MB')
print('shape before squeeze:', nir.shape)
print('chunks before squeeze:', nir.chunks )
nir = nir.squeeze()
print('shape after squeeze:', nir.shape)
print('chunk after squeeze:', nir.chunks )
```

### Calculating NDVI
Next we set up the NDVI calculation. 
This step is easy because we can handle xarrays and Dask arrays as numpy arrays for arithmetic operations. 
Also, both bands have values of type float32, so we won't have trouble with the division. 

```{python}
ndvi = (nir - red) / (nir + red)
```

When we look at the ndvi we can see the result is another `dask.array`, nothing has been computed yet. 
Remember Dask computations are lazy, so we need to call `compute()` to actually bring the results to memory. 

```{python}
ndvi_values = ndvi.compute()
```

And finally we can see how these look like. 
Notice that `xarray` uses the value of the dimensions as labels along the x and y axes. 
We use `robust=True` to ignore the no-data values when plotting.

```{python}
#| eval: false
ndvi_values.plot(robust=True)
```

![](../images/dask_ndvi_graph.png)
## Best Practices

It is important to remember that, while APIs may be similar, some differences do exist. Additionally, the performance of some algorithms may differ from their in-memory counterparts due to the advantages and disadvantages of parallel programming. Some thought and attention is still required when using Dask.
https://docs.dask.org/en/latest/user-interfaces.html

:::{.callout-warning}
For data that fits into RAM, Pandas can often be faster and easier to use than Dask DataFrame. 
While “Big Data” tools can be exciting, they are almost always worse than normal data tools while those remain appropriate.
https://docs.dask.org/en/stable/dataframe-best-practices.html
:::

    - https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask
    - https://coiled.io/blog/common-dask-mistakes/

Overhead

Choosing chunking