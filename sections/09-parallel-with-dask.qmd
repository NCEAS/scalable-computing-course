---
title: "Parallelization with Dask"
---
## Introduction
Dask is a library for parallel computing in Python. 
It can scale up code to use the full capacity of your personal computer or to distribute work in a cloud cluster. 
By mirroring APIs of other commonly used Python libraries such as Pandas, NumPy, and Scikit-learn, Dask provides a familiar interface that makes it easier to parallelize your code. 
In this lesson, we will get acquainted with some of Dask's most commonly used objects and Dask's way of distributing and evaluating computations. 

![ ](../images/dask_logo.png)

## Objectives

- Become familiar with Dask processing workflow:
    - What are the client, scheduler, workers, and cluster
    - https://www.datarevenue.com/en-blog/understanding-dask-architecture-client-scheduler-workers
    - Understand delayed computations and "lazy" evaluation
    - Obtain information about computations via the Dask dashboard
    - https://www.youtube.com/watch?v=N_GqzcuGLCY
    
- Learn basics of `dask.arrays` and `dask.dataframes`:
    - Load data and specifying chunk sizes
    - https://saturncloud.io/blog/a-data-scientist-s-guide-to-lazy-evaluation-with-dask/
    - Interpret a task graph

- Integrate `xarray` with Dask:
    - https://docs.xarray.dev/en/stable/user-guide/dask.html
    
- Share best practices and resources for a deeper dive
    - https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask
    - https://coiled.io/blog/common-dask-mistakes/

## Possible computing examples:
https://docs.dask.org/en/latest/10-minutes-to-dask.html 

https://tutorial.dask.org/00_overview.html


## Dask Cluster
Dask clusters have three main components for processing computations in parallel. These are the *client*, the *scheduler* and the *workers*.

- When we code, we communicate directly with the **client**, which is responsible for submitting tasks to be executed to the scheduler.

- After receiving the tasks from the client, the **scheduler** determines how tasks will be distributed among the workers and coordinates them to process tasks in parallel.

- Finally, the **workers** are threads, processes, or separate machines in a cluster. They compute tasks and store and return computations results.

In order to interact with the client and generate tasks that can be processed in parallel we need to use Dasks' objects to read our data. Here we will see examples of how to use `dask.dataframes` and `dask.arrays`.

![From: https://www.datarevenue.com/en-blog/understanding-dask-architecture-client-scheduler-workers Make this into a collored graph, add cluster envelope ](../images/dask_cluster.png)

:::{.callout-note collapse=True}
We can deploy a Dask cluster on a single machine or an actual cluster with multiple machines. Here we will use the Dask cluster instead of the default Dask scheduler to take advantage of the *cluster Dashboard*, which keeps track of the performance and progress of our computations. [Read more about Dask Clusters](https://docs.dask.org/en/latest/deploying.html)
:::

```{python}
from dask.distributed import LocalCluster, Client
```

```{python}
cluster = LocalCluster()
cluster
```
## Dask Data Frames


When we analyze tabular data, we usually start our analysis by loading it into memory as a Pandas DataFrame. 
But, what if this data does not fit in memory? 
Or maybe our analyzes crash because we run out of memory. 
These scenarios are typical entry points into parallel computing. 
In such cases, Dask’s scalable alternative to a Pandas DataFrame is the `dask.dataframe`. 
A `dask.dataframe` is made up of many `pd.DataFrames`, each containing a subset of rows of the original dataset.
We call each of these pandas pieces a *partition* of the `dask.dataframe`.



![ Dask Array design ([dask documentation](https://docs.dask.org/en/latest/dataframe.html))](../images/dask_dataframe.png)


### Reading csv as `dask.dataframe`
To get familiar with `dask.dataframes` we will use tabular data of soil mositure measurements at six forest stands in northeastern Siberia. 
The data has been collected since 2014 and is archived at the Arctic Data Center ([Loranty & Alexander, doi:10.18739/A24B2X59C](https://arcticdata.io/catalog/view/doi%3A10.18739%2FA24B2X59C)).
Just as we did on the previous lesson, we download the data using the `requests` package and the url to the data obtained from the Arctic Data Center website. 

```{python}
import os              
import requests 
```

```{python}
url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A27e4043d-75eb-4c4f-9427-0d442526c154'

response = requests.get(url)
open("dg_soil_moisture.csv", "wb").write(response.content)

```

In the Arctic Data Center metadata we can see this file is 115MB. 
To import this data as `dask.dataframe` with more than one partition we need to specify the size of each partition with the `blocksize` parameter. 
In this example we will split the data frame into six partitions, which would mean a blocksize of approximately 20 MB.

:::{.callout-note collapse=True}
*About the encoding parameter:*  
If we try to import the file directly we will obtain the error `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb3 in position 192:` `invalid start byte`. 
This means XXXX.
To find the encoding of the file and add the appropiate encodign to `dask.dataframe.read_csv` we can run the following code.
```{python}
# import chardet
# fp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')
# with open(fp, 'rb') as rawdata:
#     result = chardet.detect(rawdata.read(100000))
# result

```
:::

```{python}
import dask.dataframe as dd

fp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')
df = dd.read_csv(fp, blocksize = '20MB' , encoding='ISO-8859-1')
df
```

Notice that we cannot see any values in the data frame. 
This is because dask has not really loaded the data, it will wait until we explicitely ask it to print or compute something with the dataframe to start handling the data. 
However, we can still do `df.head()`, it's fast and not costly for memory to just access a few rows of the data frame. 

```{python}
df.head()
```

### Lazy Computations & Task Graph
The application programming interface (API) of a `dask.dataframe` is a subset of the `pd.DataFrame` API.
So if you are familiar with pandas, many of the core `pd.DataFrame` methods directly translate to `dask.dataframes`. 

A major difference between both objects is that `dask.datagrames` are *lazy*, meaning that they do not evaluate until we explicitly ask for a result using the `compute` method.
**This is the case with most Dask workloads.**

```{python}
## THIS IS AN EXAMPLE OF LAZY COMPUTATION

```


*This means that your framework will queue up sets of transformations or calculations so that they are ready to run later, in parallel. This is a concept you’ll find in lots of frameworks for parallel computing, including Dask. Your framework won’t evaluate the requested computations until explicitly told to. This differs from “eager” evaluation functions, which compute instantly upon being called.*

https://saturncloud.io/blog/a-data-scientist-s-guide-to-lazy-evaluation-with-dask/ 
Lead into task graphs. 
```{python}
### THIS IS AN EXAMPLE OF TASK GRAPH
```




## Dask Arrays
If you want scalable NumPy arrays, then start with Dask array


![ Dask Array design ([dask documentation](https://docs.dask.org/en/latest/array.html))](../images/dask_array.png)

## Best Practices

It is important to remember that, while APIs may be similar, some differences do exist. Additionally, the performance of some algorithms may differ from their in-memory counterparts due to the advantages and disadvantages of parallel programming. Some thought and attention is still required when using Dask.
https://docs.dask.org/en/latest/user-interfaces.html

:::{.callout-warning}
For data that fits into RAM, Pandas can often be faster and easier to use than Dask DataFrame. 
While “Big Data” tools can be exciting, they are almost always worse than normal data tools while those remain appropriate.
https://docs.dask.org/en/stable/dataframe-best-practices.html
:::


## TODO

- Find dataset(s) to use:
    - https://arcticdata.io/catalog/view/doi%3A10.18739%2FA24B2X59C
        "Understory micrometorology across a larch forest density gradient in northeastern Siberia 2014-2020"
        ~50MB, has a few different axes to filter on
    - https://arcticdata.io/catalog/view/doi%3A10.18739%2FA28W38388
        "River and lake ice phenology data for Alaska and Northwest Canada from 1882 to 2021"

### Bryce's Notes

- https://www.dask.org/
- https://docs.xarray.dev/en/stable/user-guide/dask.html#dask
- https://stephanhoyer.com/2015/06/11/xray-dask-out-of-core-labeled-arrays/
- https://examples.dask.org/xarray.html
- Good example to base exercise on: https://examples.dask.org/applications/image-processing.html

- Split-apply-combine
- Dask stuff
    - Lazy eval (compute())
        - Grouping compute() calls versus calling compute() multiple times
    - Dask Array
    - Dask DataFrame
    - Skip or just mention Bag, Delayed, Futures? Not sure yet.
    - visualize()
    - Choosing how many chunks to divide work into
    - Task overhead
    - Distributed dask?
        - Persist
> Dask is convenient on a laptop. It installs trivially with conda or pip and extends the size of convenient datasets from “fits in memory” to “fits on disk”.

-- From https://docs.dask.org/en/stable/