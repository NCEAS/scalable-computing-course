---
title: "Parallelization with Dask"
---
## Introduction
Dask is a library for parallel computing in Python. 
It can scale up code to use the full capacity of your personal computer or to distribute work in a cloud cluster. 
By mirroring APIs of other commonly used Python libraries such as Pandas, NumPy, and Scikit-learn, Dask provides a familiar interface that makes it easier to parallelize your code. 
In this lesson, we will get acquainted with some of Dask's most commonly used objects and Dask's way of distributing and evaluating computations. 

![ ](../images/dask_logo.png)

## Objectives

- Become familiar with Dask processing workflow:
    - What are the client, scheduler, workers, and cluster
    - Understand delayed computations and "lazy" evaluation
    - Obtain information about computations via the Dask dashboard
    - https://www.youtube.com/watch?v=N_GqzcuGLCY
    
- Learn basics of `dask.arrays` and `dask.dataframes`:
    - Load data and specifying chunk sizes
    - Interpret a task graph

- Integrate `xarray` with Dask:
    
- Share best practices and resources for a deeper dive
    - https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask
    - https://coiled.io/blog/common-dask-mistakes/


## Dask Cluster
We can deploy a Dask cluster on a single machine or an actual cluster with multiple machines. 
Here we have chosen to use the Dask cluster instead of the default Dask scheduler to take advantage of the *cluster dashboard*, which keeps track of the performance and progress of our computations. 

Dask clusters have three main components for processing computations in parallel. These are the *client*, the *scheduler* and the *workers*.

- When we code, we communicate directly with the **client**, which is responsible for submitting tasks to be executed to the scheduler.

- After receiving the tasks from the client, the **scheduler** determines how tasks will be distributed among the workers and coordinates them to process tasks in parallel.

- Finally, the **workers** are threads, processes, or separate machines in a cluster. They compute tasks and store and return computations results.

In order to interact with the client and generate tasks that can be processed in parallel we need to use Dasks' objects to read our data. Here we will see examples of how to use `dask.dataframes` and `dask.arrays`.

![From: https://www.datarevenue.com/en-blog/understanding-dask-architecture-client-scheduler-workers Make this into a collored graph, add cluster envelope ](../images/dask_cluster.png)


### Setting up a Local Cluster
We can create a local cluster as follows:

```{python}
from dask.distributed import LocalCluster, Client
# https://distributed.dask.org/en/stable/install.html
```

```{python}
# cluster = LocalCluster()
# cluster
```

And then we create a client to connect to our cluster:

```{python}
# client = Client(cluster)
# client
```

[Read more about Dask clusters](https://docs.dask.org/en/latest/deploying.html)

## `dask.dataframes`
When we analyze tabular data, we usually start our analysis by loading it into memory as a Pandas DataFrame. 
But, what if this data does not fit in memory? 
Or maybe our analyzes crash because we run out of memory. 
These scenarios are typical entry points into parallel computing. 
In such cases, Dask’s scalable alternative to a Pandas DataFrame is the `dask.dataframe`. 
A `dask.dataframe` is made up of many `pd.DataFrames`, each containing a subset of rows of the original dataset.
We call each of these pandas pieces a *partition* of the `dask.dataframe`.



![ Dask Array design ([dask documentation](https://docs.dask.org/en/latest/dataframe.html))](../images/dask_dataframe.png)


### Reading a csv
To get familiar with `dask.dataframes` we will use tabular data of soil mositure measurements at six forest stands in northeastern Siberia. 
The data has been collected since 2014 and is archived at the Arctic Data Center ([Loranty & Alexander, doi:10.18739/A24B2X59C](https://arcticdata.io/catalog/view/doi%3A10.18739%2FA24B2X59C)).
Just as we did on the previous lesson, we download the data using the `requests` package and the url to the data obtained from the Arctic Data Center website. 

```{python}
import os              
import requests 
```

```{python}
url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A27e4043d-75eb-4c4f-9427-0d442526c154'

response = requests.get(url)
open("dg_soil_moisture.csv", "wb").write(response.content)

```

In the Arctic Data Center metadata we can see this file is 115MB. 
To import this data as `dask.dataframe` with more than one partition we need to specify the size of each partition with the `blocksize` parameter. 
In this example we will split the data frame into six partitions, which would mean a blocksize of approximately 20 MB.

:::{.callout-note collapse=True}
*About the encoding parameter:*  
If we try to import the file directly we will obtain the error `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb3 in position 192:` `invalid start byte`. 
This means XXXX.
To find the encoding of the file and add the appropiate encodign to `dask.dataframe.read_csv` we can run the following code.
```{python}
# import chardet
# fp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')
# with open(fp, 'rb') as rawdata:
#     result = chardet.detect(rawdata.read(100000))
# result

```
:::

```{python}
import dask.dataframe as dd

fp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')
df = dd.read_csv(fp, blocksize = '20MB' , encoding='ISO-8859-1')
df
```

Notice that we cannot see any values in the data frame. 
This is because Dask has not really loaded the data, it will wait until we explicitely ask it to print or compute something to do so. 
However, we can still do `df.head()`, it's not costly for memory to just access a few rows of the data frame. 

```{python}
df.head()
```

### Lazy Computations & Task Graph
The application programming interface (API) of a `dask.dataframe` is a subset of the `pd.DataFrame` API.
So if you are familiar with pandas, many of the core `pd.DataFrame` methods directly translate to `dask.dataframes`. 

A major difference between both objects is that `dask.dataframes` are *lazy*, meaning that they do not evaluate until we explicitly ask for a result using the `compute` method.
This is the case with most Dask workloads, they do **lazy computations**.

```{python}

```


*This means that your framework will queue up sets of transformations or calculations so that they are ready to run later, in parallel. 

This differs from “eager” evaluation functions, which compute instantly upon being called.*
https://saturncloud.io/blog/a-data-scientist-s-guide-to-lazy-evaluation-with-dask/

Lead into task graphs. 
```{python}
### THIS IS AN EXAMPLE OF TASK GRAPH

##!! cannot run graph after installing graphviz with
#pip install graphviz

# Error:
# ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH

# See:
#https://stackoverflow.com/questions/42014458/dask-not-installing-graphviz-dependency
```




## `dask.arrays`
Another common object we might want to parallelize is a NumPy array. 
The equivalent Dask object is the `dask.array`, which coordinates many NumPy arrays that may live on disk or other machines. 
Each of these NumPy arrays within the `dask.array` is called a **chunk**. 
Choosing how these chunks are arranged within the `dask.array` and their size can significantly affect the performance of our code. 
Here you can find more information about [chunks](https://docs.dask.org/en/latest/array-chunks.html) and [best practices](https://docs.dask.org/en/latest/array-best-practices.html).


![ Dask Array design ([dask documentation](https://docs.dask.org/en/latest/array.html))](../images/dask_array.png)


```{python}
import dask.array as da
import numpy as np
```


```{python}
data = np.arange(100_000).reshape(200, 500)
a = da.from_array(data, chunks=(100, 100))
a
```

Indexing Dask collections feels just like slicing NumPy arrays:

```{python}
a[:50, 200]
```

And computations also work lazily:

```{python}
a.mean()
```

```{python}
a.mean().compute()
```

## Dask and xarray
Going forward, it might be more common having to read some big array-like dataset (like a high-resolution multiband raster) than creating one from scratch from a NumPy array. 
In this case it can be useful to use the `xarray` module together with Dask. 
It is simple to wrap Dask around `xarray` objetcs, we only need to specify the number of chunks argument as an argument when we are reading in a dataset (see also [[1]](https://docs.xarray.dev/en/stable/user-guide/dask.html)).

### Open .tif file
As an example, let's do an NDVI calculation using remote sensing imagery collected by aerial vehicles at a series of fire perimeters in larch forests in northeastern Siberia (Loranty, Forbath, Talucci, Alexander, DeMarco, et al. 2020. Uncrewed aerial vehicle remote sensing imagery of postfire vegetation in Siberian larch forests 2018-2019. [Arctic Data Center. doi:10.18739/A2ZC7RV6H](https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2ZC7RV6H).). 

First we download the data for the near-infrared (NIR) and red bands:

```{python}
# download red band
url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Aac25a399-b174-41c1-b6d3-09974b161e5a'
response = requests.get(url)
open("RU_ANS_TR2_FL005M_red.tif", "wb").write(response.content)

# download nir band
url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A1762205e-c505-450d-90ed-d4f3e4c302a7'
response = requests.get(url)
open("RU_ANS_TR2_FL005M_nir.tif", "wb").write(response.content)
```


Because these are .tif files, we will use `xarray.open_rasterio` to read them.
To indicate we will open these files with `dask.arrays` as the underlying object to the `xarray.DataArray` (instead of a `numpy.array`) we need to specify either a shape or the size in bytes for each chunk. 
Both files are 76 MB, so let us have chunks of 15 MB to have roughly 6 chunks.


```{python}
import xarray as xr
```

```{python}
# read in the file
fp_red = os.path.join(os.getcwd(),"RU_ANS_TR2_FL005M_red.tif")
red = xr.open_rasterio(fp_red, chunks = '15MB')
red
```

We can see a lot of useful information here:

- There are 8 chunks in the array. We were aiming for six, but this often happens with the way Dask distributes the memory (76MB is not exactly divisible by 6).
- Geospatial information (transformation, CRS, resolution) and no-data values.
- There is an unnecessary dimension because as a constant value for the band. So our next step is to squeeze the array to flatten it.

```{python}
# getting rid of unnecessary dimension
red = red.squeeze()
red
```

Next we read in the NIR band and do the same pre-processing:
```{python}
fp_nir = os.path.join(os.getcwd(),"RU_ANS_TR2_FL005M_nir.tif")
nir = xr.open_rasterio(fp_nir, chunks = '15MB')
print('shape before squeeze:', nir.shape)
nir = nir.squeeze()
print('shape after squeeze:', nir.shape)
print('chunks:', nir.chunks )
```

### Calculating NDVI
Next we set up the NDVI calculation, this step is ease because we can handle xarrays and dask arrays as numpy arrays for arithmetic operations. 
Also, both bandas have values of type float32, so we won't have trouble with the division. 

```{python}
ndvi = (nir - red) / (nir + red)
ndvi
```

When we look at the ndvi we can see the result is another `dask.array`, nothing has been computed yet. 
We need to call `compute()` to actually bring the results to memory. 

```{python}
ndvi_values = ndvi.compute()
ndvi_values
```

And finally we can see how these look like. 
Notice that `xarray` uses the value of the dimensions as labels along the x and y axes. 
We use `robust=True` to ignore the nodatavalues when plotting.
```{python}
#| eval: false
ndvi_values.plot(robust=True)
```

## Best Practices

It is important to remember that, while APIs may be similar, some differences do exist. Additionally, the performance of some algorithms may differ from their in-memory counterparts due to the advantages and disadvantages of parallel programming. Some thought and attention is still required when using Dask.
https://docs.dask.org/en/latest/user-interfaces.html

:::{.callout-warning}
For data that fits into RAM, Pandas can often be faster and easier to use than Dask DataFrame. 
While “Big Data” tools can be exciting, they are almost always worse than normal data tools while those remain appropriate.
https://docs.dask.org/en/stable/dataframe-best-practices.html
:::

Overhead

Choosing chunking