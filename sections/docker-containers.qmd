---
title: "Docker Containers"
from: markdown+emoji
---

## Learning Objectives

- What is docker anyway?
- Think about dependency management, reproducibility, and software
- Become familiar with containers as a tool to improve computational reproducibility
- Build and run docker containers to create reproducible python environments

## Just what is a container?

And why might I want one? In short, as a very lightweight way to create a reproducible computing environment. Containers have some desirable properties:

- **Declarative**: The are defined by a clear procedure making it possible to reliably rebuild an identical container in a variety of environments, promototing reproducibility
- **Portable**: Containers are designed to run on a container runtime which works identically across systems, so you can launch a container from a Mac, Linux, or Windows and get the same output every time
- **Lightweight**: Containers contain only the exact files and data needed for a specific application, without all of the extra operating system baggage found in virtual machines
- **Scalable**: Containers can be launched multiple times from their source image, and thus can be horizontally scaled across a compute cluster to parallelize operations

From the following figure, one can see that a container is much more lightweight than both a bare metal server and a virtual machine, in that it contains only the files needed for a specific application.

![](../images/virtualization.png)

**Images** *An image is a snapshot of a computing environment.* It contains all of the files and data needed to execute a particular application or service, along with the instructions on which service should be run. But it is not executed per se. As a snapshot, an image represents a template that can be used to create one or more containers (each of which is an instantiation of the contents of the image). Images are also built using a layered file system, which allows multiple images to be layered together to create a composite that provides rich services without as much duplication. 

**Containers** *A container represents in instance of an image that can be run.* Containers are executed in a Container Runtime such as [`containerd`](https://containerd.io/) or [Docker Engine](https://docs.docker.com/engine/). Like virtual machines, containers provide mechanisms to create images that can be executed by a container runtime, and which provide stronger isolation among deployments. But they are also more lightweight, as the container only contains the libraries and executables needed to execute a target application, and not an entire guest operating system. This means that applications run with fewer resources, start up and shut down more quickly, and can be migrated easily to other hosts in a network.

::: {.callout-note}

::: {layout="[[40,60]]"}

![](../images/docker-small-logo.png)

The [Docker](https://docker.com) system was one of the first widespread implementations of containers, and its popularity drove much of the terminology around containers. After it's inception, the software ecosystem around contaianers has been standardized and maintained by the [Open Container Intitiative (OCI)](https://opencontainers.org), which defines the image specification, runtime specification, and distribution specification followed by most vendors. So, we might often use the 'docker' terminology, but we are referring to OCI-compliant images and containers.

:::

:::

## Hands-on with Containers and Docker

Let's get started. At it's simplest, you can use docker to run an image that somebody else created. Let's do that with a simple Hello World app. Open a terminal (on a machine with docker installed), and run:

```bash
$ docker run hello-world
```

```txt
Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (arm64v8)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/
```

In a nutshell, the `docker run` command will take an image from your local machine, and execute it as a container instance.

![](../images/docker-run.png)

You can manage your local containers and images using the docker client. Start by listing your local images:

:::{.column-page-right}
```bash
$ docker image ls
REPOSITORY                                          TAG       IMAGE ID       CREATED         SIZE
hello-world                                         latest    ee301c921b8a   10 months ago   9.14kB
```
:::
And you can list the containers that are running. Because containers are somewhat analogous to processes, list them with:

:::{.column-page-right}
```bash
$ docker ps
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
```
:::
There is no output there, because no containers are currently running. You can see the containers that were run previously by adding the `-a` (all) option. Only long-running containers will generally be visible with `docker ps` without the `-a` flag.

:::{.column-page-right}
```bash
$ docker ps -a
CONTAINER ID   IMAGE         COMMAND    CREATED          STATUS                      PORTS     NAMES
f0a2c6c27af7   hello-world   "/hello"   12 minutes ago   Exited (0) 12 minutes ago             nervous_vaughan
```
:::
So from that listing, we can see the `hello-world` image (with id `f0a2c6c27af7`) was run 12 minutes ago. We could run it again with either the container identifier (using `docker start -i f0a2c6c27af7`), or with the container name, which was assigned automatically (`docker start -i nervous_vaughan`).  Using `start`, we are running the same container instance (`f0a2c6c27af7`) as previously, rather than creating a new one (which we could do by calling `docker run` again).

Once you are finished with a container you can remove the container, but note that the image will still be present locally.

:::{.column-page-right}
```bash
$ docker ps -a
CONTAINER ID   IMAGE         COMMAND    CREATED          STATUS                     PORTS     NAMES
f0a2c6c27af7   hello-world   "/hello"   22 minutes ago   Exited (0) 6 minutes ago             nervous_vaughan

$ docker rm f0a2c6c27af7
f0a2c6c27af7

$ docker ps -a
CONTAINER ID   IMAGE         COMMAND    CREATED      STATUS                  PORTS     NAMES

$ docker image ls
REPOSITORY                                          TAG       IMAGE ID       CREATED         SIZE
hello-world                                         latest    ee301c921b8a   10 months ago   9.14kB
```
:::

So the image is still on your local machine and could be run again to create a new container anytime.

## How about python?

::: {layout="[[70,30]]"}
We've been using python 3.10, which is installed on the server. What if you wanted to run a newer version of python? It's a command away with docker:

![](../images/python-logo.png)
:::

:::{.column-page-right}
```bash
$ docker run -it python:3.12
Unable to find image 'python:3.12' locally
3.12: Pulling from library/python
6ee0baa58a3d: Pull complete 
992a857ef575: Pull complete 
3861a6536e4e: Pull complete 
e5e6faea05ea: Pull complete 
91c9495e7b5a: Pull complete 
9001688a971d: Pull complete 
ad27ab4515af: Pull complete 
b152d3b07485: Pull complete 
Digest: sha256:336461f63f4eb1100e178d5acbfea3d1a5b2a53dea88aa0f9b8482d4d02e981c
Status: Downloaded newer image for python:3.12

Python 3.12.2 (main, Mar 12 2024, 08:01:18) [GCC 12.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> print("Hello from python!")
Hello from python!
>>> 
```
:::

You can even use this image to run commands at the command prompt. The `-i` interactive and `-t` terminal options allow you to run a command interactively with terminal output.  Let's run a quick command to print the python version:

:::{.column-page-right}
```bash
$ docker run -it --rm python:3.12 python -V
Python 3.12.2
```
:::

and we could run a little python command-line calculation from the image:

:::{.column-page-right}
```bash
$ docker run -it --rm python:3.12 python -c 'print(f"Four plus one is {4+1}")'
Four plus one is 5
```
:::

If you were to run commands like this frequently, you'd build up a lot of container instances from your image, so instead we passed the `--rm` option to tell docker to delete the container (but not the image) when it is done executing each time.  So, at times you'll need to clean up. I've run the commands a few times interactively without remembering the `--rm` option, so now I can list and remove the containers that I don't need hanging around any more.

:::{.column-page-right}
```bash
$ docker ps -a
CONTAINER ID   IMAGE         COMMAND     CREATED             STATUS                           PORTS     NAMES
d3f717354ed5   python:3.12   "python3"   About an hour ago   Exited (0) 59 minutes ago                  laughing_borg
e882d6a89f2c   python:3.12   "python3"   About an hour ago   Exited (130) About an hour ago             interesting_elgamal
9d984d596e4e   python:3.12   "python3"   About an hour ago   Exited (0) About an hour ago               adoring_jang
b015e761fbab   hello-world   "/hello"    7 days ago          Exited (0) 7 days ago                      compassionate_wozniak
1bf3a36a924c   hello-world   "/hello"    7 days ago          Exited (0) 7 days ago                      jolly_goldwasser

$ docker rm laughing_borg interesting_elgamal adoring_jang compassionate_wozniak
laughing_borg
interesting_elgamal
adoring_jang
compassionate_wozniak

$ docker ps -a
CONTAINER ID   IMAGE         COMMAND    CREATED      STATUS                  PORTS     NAMES
1bf3a36a924c   hello-world   "/hello"   7 days ago   Exited (0) 7 days ago             jolly_goldwasser
```
:::

## Exploring image registries

Now that version of python is available on your machine. When you list the image, you'll see that the python version is provided as the `tag`.

:::{.column-page-right}
```bash
$ docker image ls
REPOSITORY                                          TAG       IMAGE ID       CREATED         SIZE
python                                              3.12      ff82813aa87b   6 weeks ago     1.02GB
hello-world                                         latest    ee301c921b8a   10 months ago   9.14kB
```
:::

The `TAG` associated with an image is typically used to indicate the version of the image. We downloaded this image from the DockerHub image registry. If you inspect the [DockerHub python page](https://hub.docker.com/_/python), you'll see that this is an 'official' image from the python project, and that it is one of many different images that can be downloaded. So, images are a fantastic way to be able to reliably return to a specific version of software that you might want to use.

::: {.column-margin}

::: {.callout-caution}
Be careful with images that you download from the internet -- they might contain anything, including malware. Because you are running the code locally, you never know what mischief might be managed, and you probably don't want to find out. So it is best to only run images from trusted sources. Generally, the 'official' images for projects are marked in image registries, and there are procedures to ensure they are from those trusted sources. But, it's also easy to spoof something that looks official, so put on your tinfoil hat when running unfamiliar images.
:::

:::

![](../images/dockerhub-python.png)

Of course, python is just the start. You can find images for just about any software you need, including machine learning libraries like **TensorFlow** ([tensorflow/tensorflow](https://hub.docker.com/r/tensorflow/tensorflow)), databases like **Postresql** ([bitnami/postgresql](https://hub.docker.com/r/bitnami/postgresql)), search systems like **Solr** ([solr](https://hub.docker.com/_/solr)), and web servers like **nginx** ([nginx](https://hub.docker.com/_/nginx)).  Take a few mintues to explore the thousands of applications available at these image registries:


:::{layout-ncol="2"}

[![](../images/dockerhub-logo.png)](https://hub.docker.com)

[![](../images/artifacthub-logo.png)](https://artifacthub.io/)

:::

## Custom images with Dockerfiles

While pre-built images are highly useful, even more importantly, you can build your own custom images with the precise collection of software needed to run an analysis, a model, or visualize your data. This use of images to build custom containers creates portable and highly repeatible analyses because it allows us to declare the exact set of software needed in your computing environment. Basically, it allows you to take your pre-defined computing environment anywhere needed for execution, form your local laptop to the largest supercomputers in the world.

:::{layout-ncol="2"}

![](../images/docker-build.png)

Images can be built by declaring their contents in a `Dockerfile`, which is a simple text file containing instructions on how to build the image and what it contains. In general, you can customize an existing image by adding folders and files contianing software and data that you need. To create an image, you can start by creating a `Dockerfile` that contains a `FROM` directive listing an existin image you want to build from. In our case, let's start by building an image based on the stock Ubuntu operating system.

::: 

```Dockerfile
# Dockerfile for ADC Scalable Computing Course
FROM ubuntu:22.04
```

In that file, the `FROM` command is the first directive, and it indicates that we want to build an image that extends the existing `ubuntu:22.04` image. So our image will start with everything that is present in Ubuntu. Technically, that is all we need to get started. Let's build and run the image to see what's there.

:::{.column-page-right}
```bash
$ docker build -t adccourse:0.1 .
[+] Building 0.0s (5/5) FINISHED                                                                                                                                    docker:default
 => [internal] load .dockerignore                                                                                                                                             0.0s
 => => transferring context: 2B                                                                                                                                               0.0s
 => [internal] load build definition from Dockerfile                                                                                                                          0.0s
 => => transferring dockerfile: 191B                                                                                                                                          0.0s
 => [internal] load metadata for docker.io/library/ubuntu:22.04                                                                                                               0.0s
 => CACHED [1/1] FROM docker.io/library/ubuntu:22.04                                                                                                                          0.0s
 => exporting to image                                                                                                                                                        0.0s
 => => exporting layers                                                                                                                                                       0.0s
 => => writing image sha256:7dbc68b81268657c977e24cfe712e6907f1cb76c08aaf6397f94a3149db11069                                                                                  0.0s
 => => naming to docker.io/library/adccourse:0.1                                                                                                                              0.0s

$ docker run -it adccourse:0.1
root@76529c3b3358:/# pwd
/
root@76529c3b3358:/# ls
bin  boot  dev  etc  home  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@76529c3b3358:/# exit
```
:::

:tada: **We have a working image, declared from our Dockerfile!** :tada:

### Adding software 

Now let's extend it to add some software we might need. For example, let's add python and some utilities. First, we'll use the `SHELL` directive to indicate that we want all future commmands from the Dockerfile to be run using the Bash shell, and the `WORKDIR` directive to set up our working directory to a better location than `/`. In this case, we will be building an application for the scalable computing (scomp) course, so we'll put our working files in a typical linux HOME directory location, `/home/scomp`.

The `RUN` directive can be used to run any shell command that is needed to modify the image. In this example, we will use it to run the `apt update` and `apt install` commands to install the python package and some standard utilities on the system. Note how we use the `&&` operator to combine two bash commands into a single `RUN` invocation. When using `apt` in an image, you typically need to run `apt update` first to get the full list of software package sources that can be installed.

```Dockerfile
# Dockerfile for ADC Scalable Computing Course
FROM ubuntu:22.04
SHELL ["/bin/bash", "-c"]
WORKDIR /home/scomp
RUN apt update && apt -y install python3 pip virtualenvwrapper vim nano iproute2
```

Build it again with `docker build -t adccourse:0.2 .`, and then let's inspect it by running `docker run -it adccourse:0.2`.  Note now that our working directory is `/home/scomp`, and you can now run the python3 command, which shows we have python 3.10 installed:

```bash
$ docker run -it adccourse:0.2
root@e9566a19e24b:/home/scomp# pwd
/home/scomp
root@e9566a19e24b:/home/scomp# python3 -V
Python 3.10.12
root@e9566a19e24b:/home/scomp# exit
```

When we''ve been running these containers, we have not been using the `--rm` flag from before. So, you'll note that they are building up in our container list. We can clean them up with `docker rm` as we don't really intend to reuse these containers as we work through building up our image. And let's use `--rm` for future runs to prevent this from happening.

:::{.column-page-right}
```bash
$ docker ps -a
CONTAINER ID   IMAGE           COMMAND       CREATED          STATUS                            PORTS     NAMES
e9566a19e24b   adccourse:0.2   "/bin/bash"   3 minutes ago    Exited (127) About a minute ago             affectionate_swanson
d493605cfb60   adccourse:0.2   "/bin/bash"   5 minutes ago    Exited (0) 4 minutes ago                    sharp_elbakyan
a9ab223669ac   adccourse:0.2   "/bin/bash"   5 minutes ago    Exited (127) 5 minutes ago                  wonderful_chaum
76529c3b3358   adccourse:0.1   "/bin/bash"   22 minutes ago   Exited (0) 8 minutes ago                    heuristic_nash
1bf3a36a924c   hello-world     "/hello"      8 days ago       Exited (0) 8 days ago                       jolly_goldwasser

$ docker rm e9566a19e24b d493605cfb60 a9ab223669ac 76529c3b3358
e9566a19e24b
d493605cfb60
a9ab223669ac
76529c3b3358
```
:::

### Add a user account

When we ran our images previously, we noted that the image was running as the `root` user, and the default working directory was `/`. By setting `WORKDIR`, we are now working within the `/home/scomp` directory, but still as the user `root`. Best practice would be to create a dedicated user account that doesn't have the administrative priveledges of root. So, we'll create an account `scomp` and group `scomp` that will own all of the files we create and will run our processes. 

```Dockerfile
# Dockerfile for ADC Scalable Computing Course
FROM ubuntu:22.04
SHELL ["/bin/bash", "-c"]
WORKDIR /home/scomp
RUN apt update && apt -y install python3 pip virtualenvwrapper vim nano iproute2
RUN groupadd -r scomp && useradd -r -g scomp scomp
RUN mkdir -p /home/scomp && chown scomp.scomp /home/scomp
USER scomp:scomp
```

Rebuild the image (`docker build -t adccourse:0.3 .`) and run it, and you'll see that we are now running as the `scomp` user in the `/home/scomp` directory. This time, when we run it, we'll also use the `-h` hostname option to create a bit more readable hostname, rather than the container identifier.

```bash
$ docker run -it --rm -h adccourse adccourse:0.3
scomp@adccourse:~$ pwd
/home/scomp
scomp@adccourse:~$ whoami
scomp
scomp@adccourse:~$ exit
```

### Add a python venv

Now that we have a working image with python installed, lets configure the image to create a standardized python virtual environment with the python packages that we'll need in our application. Start by creating a `requirements.txt` file in the same directory as your `Dockerfile`, with the list of packages needed. We'll start with just one, `xarray`. 

```bash
$ echo "xarray==2024.2.0" >> requirements.txt
$ cat requirements.txt
xarray==2024.2.0
```

To create the virtualenv, we will need to first configure virtualenvwrapper, and then `COPY` the requirements.txt file into the image, and then finally make the virtual environment using `mkvirtualenv`, `pip`, and `workon`. We'll go through these in more detail after we build the image. Let's build it, this time tagging it as version `1.0`.

```Dockerfile
# Dockerfile for ADC Scalable Computing Course
# Build with:
#     docker build -t adccourse:1.0 .
FROM ubuntu:22.04
SHELL ["/bin/bash", "-c"]
WORKDIR /home/scomp
RUN apt update && apt -y install python3 pip virtualenvwrapper vim nano iproute2
RUN groupadd -r scomp && useradd -r -g scomp scomp
RUN mkdir -p /home/scomp && chown scomp.scomp /home/scomp
USER scomp:scomp
RUN echo "source /usr/share/virtualenvwrapper/virtualenvwrapper.sh" >> /home/scomp/.bashrc
COPY ./requirements.txt .
RUN source /usr/share/virtualenvwrapper/virtualenvwrapper.sh && \
        mkvirtualenv scomp && \
        pip install --no-cache-dir --upgrade -r requirements.txt && \
        echo "workon scomp" >> /home/scomp/.bashrc
```

:::{.callout-note}
**Layers**. Each of the directives in the `Dockerfile` builds a single image layer, and they are run in order. Each layer is registered using the sha256 identifier for the layer, which enables layers to be cached. Thus, if you already have a layer with a given hash built or pulled into your local registry, then subsequent `docker build` commands can reuse that layer, rather than building them from scratch. As a result, its best practice to put the layers that change infrequently at the top of the Dockerfile, and layers that might change more frequently (such as application-specfiic commands) near the bottom. This will speed things up by maximizing the use of CACHED layers, which can be seen in the output of `docker build`.

```bash
❯ docker build -t adccourse:1.0 .
[+] Building 87.1s (13/13) FINISHED                                                                                                                docker:default
 => [internal] load build definition from Dockerfile                                                                                                         0.0s
 => => transferring dockerfile: 782B                                                                                                                         0.0s
 => [internal] load .dockerignore                                                                                                                            0.0s
 => => transferring context: 2B                                                                                                                              0.0s
 => [internal] load metadata for docker.io/library/ubuntu:22.04                                                                                              0.0s
 => [1/8] FROM docker.io/library/ubuntu:22.04                                                                                                                0.0s
 => [internal] load build context                                                                                                                            0.0s
 => => transferring context: 74B                                                                                                                             0.0s
 => CACHED [2/8] WORKDIR /home/scomp                                                                                                                         0.0s
 => CACHED [3/8] RUN apt update && apt -y install python3 pip virtualenvwrapper vim nano iproute2                                                            0.0s
 => CACHED [4/8] RUN groupadd -r scomp && useradd -r -g scomp scomp                                                                                          0.0s
 => CACHED [5/8] RUN mkdir -p /home/scomp && chown scomp.scomp /home/scomp                                                                                   0.0s
 => CACHED [6/8] RUN echo "source /usr/share/virtualenvwrapper/virtualenvwrapper.sh" >> /home/scomp/.bashrc                                                  0.0s
 => CACHED [7/8] COPY ./requirements.txt .                                                                                                                   0.0s
 => [8/8] RUN source /usr/share/virtualenvwrapper/virtualenvwrapper.sh &&         mkvirtualenv scomp &&         pip install --no-cache-dir --upgrade -r re  86.4s
 => exporting to image                                                                                                                                       0.7s
 => => exporting layers                                                                                                                                      0.7s
 => => writing image sha256:5ac62fbbb619dba0441c87b842e5ee3b254b7e08901eda7595a0860249901a19                                                                 0.0s
 => => naming to docker.io/library/adccourse:1.0                                                                                                             0.0s
 ```
:::

When we run this image, we'll now see that the `scomp` virtualenvironment was activated, and that `xarray` can be imported in the `python3` environment. By being extremely explicit about the software being installed in the Dockerfile image, we can ensure that the environment we've built is highly **portable** and **reproducible**.

```bash
$ docker run -it --rm -h adccourse adccourse:1.0
(scomp) scomp@adccourse:~$ python3
Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import xarray
>>>
(scomp) scomp@adccourse:~$ exit
```

In summary, a `Dockerfile` is a portable mechanism to declare a complete computing environment in a single text file. Using a `Dockerfile` makes it straightforward to reliably rebuild the specified environment, installing the precise software and data needed in an image. 


## Volume mounts

- containers are ephemeral -- any data they create is lost when the image finishes executing, unless you go to extra steps to preserve it
- you can get data into and out of images by mounting volumes
- think of a volume liek a thumbdrive -- when I plug it into one machine, it gets mounted there at a path, but if I plug it into a different machine, it might get mounted in a different location -- but it still contains the same content
- there are many types of volumes that can be mounted, but the simplest is a folder on the local machine
- docker makes it easy to mount a volume by specifying both which volume to mount and the path that you want it mounted at. For example, from our linux server I might take a folder from my local home directory (e.g., `/home/jones/project-data`) and mount it in another location in a docker container at (e.g., `/var/data`).
- You control the permissions on the files and folders that you mount, which determines whether the processes you run in a script can read from or write to the volume. Permission issues can easily arise when you first start using mounted volumes, but just a small amount of tweaking of your Dockerfile can set the permissions straight.
- Mount example (read-write)
    - create a directory `mkdir ~/docker-project`
    - create subdirectories for `input` and `output` and `scripts`
    - save some data into the input directory
    - create a script to open and plot the data and save it in the `scripts` directory
        - the script should save the plot to the `output` directory
    - Using the python image we built before, run the script and exit
    - From the server, inspect the `~/docker-project` directory and note the output is now present



## Sharing images to registries

::: {.columns}

::: {.column width="30%"}

![](../images/docker-pull-push.png)

:::

::: {.column width="5%"}
 
:::

::: {.column width="65%"}
Container registries...

- GHCR
- Dockerhub
- ArtifactHub
- ...
:::
:::

### Anatomy of an image

- images and layers
- why layers are important
- Dockerfiles and layers

### OCI

- layers and sha checksums
- saving layers and images

## Container lifecycle

Putting it all together, you can use the docker client tool to build an image from a `Dockerfile`, then run it as a container, and you can push that image to an image registry, or pull one down that you want to run on a particular machine.

![](../images/docker-workflow.png)

### Spinning up ubuntu

- `docker run --name container1 -h myubuntu -it ubuntu:22.04`
- `docker rm container1`

Shell on running containers
- `docker exec -it container1 -- bash`

## Exercise

- Optional?

## Bonus info: Running containers locally

Working with docker or containers requires a container runtime.  If you're on a Mac or Windows machine, you can run containers through a variety of different runtime systems. One of the nicest lately is [Rancher Desktop](https://rancherdesktop.io/). Install the binary for you platform, and then after it starts, enter the configuration Preferences, and then deselect "Kubernetes" to disable the kubernetes distribution, which takes up a lot of resources if you're not using it.

![](../images/rancher-k8s-config.png)

There are many different tools you can use with docker, including the `docker` client tool, and the `containerd` ecosystem using `nerdctl` as a client tool. Both the `docker` command and the `nerdctl` command share the same commands.  A few quick example commands one might use in the docker ecosystem:

- `docker pull python:3.9`: to grab an existing python image from the DockerHub repository
- `docker run -it python:3.9 -- python`: to start a standard python interpreter
- `docker build`: to build a new image from a Dockerfile configuration file

