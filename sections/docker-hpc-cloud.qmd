---
title: "Containers in HPC and Cloud"
---

## Learning Objectives

- Discuss containers in high performace computing environments
- Explore orchestration formats
- Learn how to use docker compose to build a workflow
- Explore a real world Kubernetes service

## Container orchestration systems

Container orchestration is the process of linking multiple containers into an integrated application. These applications can represent a computational workflow, a backend service or data store, a frontend application, or some combination of these and others.  For example, a simple application might consist of a backend database system in a container, which is accessed by containerized data processing code, and which is served by a containerized web application.

![](../images/docker-compose-workflow.png)

All of these types of integrated systems can be built by orchestrating the deployment of multiple containers, whether creating scientific applications for running big data jobs on high performance computers or building and deploying scientific web applications and services.

Let's explore a few container orchestration systems.

### docker compose

[Docker Compose](https://docs.docker.com/compose/) is "a tool for defining and running multi-container applications". This is the essence of orchestration. With just the `compose` application, you can link multiple containers into an integrated application to run it on demand. This is particularly useful to quickly bring up a backend and frontend application, say one that uses a database (like `postgresql`) or a caching server (like `redis`) and combine it with a web-based frontend.

Creating a docker compose application is done by editing a compose.yaml configuration document that describes each of the containers to be run in the system, and how they are linked together.

### swarm

### kubernetes

::: {layout="[[70,-10,20]]"}

Kubernetes is ... and there is a nice overview in the [Digital Ocean introduction to kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)

![](../images/k8s-logo.png)

:::

### Parsl

::: {layout="[[70,30]]"}

Parsl is all about the beef...

![](../images/parsl-logo.png)

:::

Remember the basic layout of a parsl app:

```python
# Define the square task.
import parsl
@python_app
def square(x):
    return x * x

# Launch four parallel square tasks.
futures = [square(i) for i in range(4)]

# Retrieve results.
squares = [future.result() for future in futures]
print(squares)
# -> [0, 1, 4, 9]
```


### Ray.io

::: {layout="[[60,40]]"}

[Ray](https://ray.io) is structured so similarly to Parsl...

![](../images/ray-logo.png)

:::

![](../images/ray-components.png)

[Ray Core](https://docs.ray.io/en/latest/ray-core/walkthrough.html) is fairly analogous to Parsl, and provides the core functionality for distributed execution. 

```python
# Define the square task.
@ray.remote
def square(x):
    return x * x

# Launch four parallel square tasks.
futures = [square.remote(i) for i in range(4)]

# Retrieve results.
print(ray.get(futures))
# -> [0, 1, 4, 9]
```

### Kubeflow

[Kubeflow](https://www.kubeflow.org/)
![](../images/kubeflow-logo.png)

```python
# Kubeflow pipeline example
from kfp import dsl

@dsl.component
def say_hello(name: str) -> str:
    hello_text = f'Hello, {name}!'
    print(hello_text)
    return hello_text

@dsl.pipeline
def hello_pipeline(recipient: str) -> str:
    hello_task = say_hello(name=recipient)
    return hello_task.output
```

## How weird is the weather?

Big data are hard to visualize, and sometimes we need to come up with clever ways to pack a lot of data into a small space. This is exactly what the **How weird is the weather?** project did ([hwitw github](https://github.com/howweirdistheweather/weather_app)), by summarizing the [ERA5 climate reanalysis data](https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5) and providing a means to visualize over 70 years of climate data interactively. This project, developed by Brentwood Higman, John McInnes, and Katmai McKittrick, shows where climate extremes have occurred over the ERA5 dataset. First, pick any location on the globe via the work-in-progress [sandcastle demo](https://sandcastle.cesium.com/#c=tVbbcts2EP0VVNNp6YlK2pn2RbLcxqodeyKPMpEvD2HGgcmViDEEcABQspPo37sASAq6uE0e+qIRsddzdrHYJCETMKQqiSmADEGzak4WDJag4lRkUmhTf5IBEbCsVeJbdxalncx9D6UwlAlQaadLvqaCEA0cMsOkuBQ5y6iRqkemlGvoWikTU3kqn9ZHq4N+E05nIACj1Vm4TxSmIknIKdVAbj6MyFQqciGX5A6YysmlJteY/h1QRLFOvFLcGQxI2imMKXUvSYolM8vfcljEVGF+WU4NjZlMtKH4lTCRw1NcmDn/M+00UYcKHUPI0FxW6BcWIAwpqMg5Iq+D1p+bdE0yBSAmJc3gzBpdeKXIgYszKhZUH+wJh6rMPONfasiScY60qgUQqp2U0wfgXpgzXXL63JybhEvR8uD1WkadUwY6pnkeuWI5hZ4vHJaukMuwWP7klGaPMyUrkfeIUVUjmmLle8jv0e/lE9IipLYgsQ28uJCKfbHNwceKzZjoNYxcbAni0dn5dW20AIW12DG53TiOr8fva/2SPQEfT6caMJWA9SFVBv9R8To6+qNLDg+c/qppN0+2FDYcoSRDbTlTtCxYRjIpsa+MxPMpes9JDhmbUyyAUUzMkNppJVx/21acUzMMrCNnzQRW8cCTqsBUSjR5XWGbxkb+DTPsCh1q4+m5DRe9tgmu6hzfMuyZuufWyrpLZnh1g4LbvqB1hkGCqDWiZiTFxEmirKGlTo5NSfRTcOiT7VuR758NZgYhu81pPFVy3tIdRAi8YIIzZqocfBroaB9zodPWYsMN3tMf81IbeCd1JT5uZdPd8vspoP/cBXiJaDeKrOzi7vL6zg6n3ebw9KMfpxNx9/lGKfpcl8BD+8+kEG1gG+L5jOeDn79u6q9+QYf2dNPt6vOPg3MT4l+Ajaz8fwGGIXpkB1maVoeHp4ddgjZOvAWxlodIb8q8Gat+IOJLUGwArzHjbCal1MzhZIbgr9WacfkAIQWVc+iR6xrX9t3yvLXeBmuF/lrsf+2QRQU7W3dkBp4MynyUANMFy2F/fQqU+NQ2MtkI5UZ84O0dAO4BCgc9kdParb9xAT0YhINxkvc1rPYqfvzUjNWJjWA9tNBrj36KhZSSZcE4ING/apQufIj6FY1xpF+KsjJvHKqohRehJszxJdvos5bc9WOX0TkoGpcsezzjnJVasjzyz0bjAh/EvIHSPHjuXfYVh8bMisJJhNA8co3hXh6y/XbGrg0Omrc2KFPfnwTTd+Xrtofotac+8ZfY8ZlxhOnXkjicmOi/MfVB0s5LyxOhJsUadsir/Vc8wOBqTTauQRAqvAu2x7ov7ULXzyXEV+Obydn91fj2rH2ZP0DOFO6Q9hX2s1W2j6DF+T1dEr5we4j89o3sOY45iJkp2hptV6Te8+wiufUI7cz5bddh++B2iobNjvoqdOi0lriLymUsSxARanWxaPcPnIrHtPMdhNp16n44uhy+s8qdbudYm2cOJx4SIX+xeSmVyyKK48TAHPdHLFfyUGWPYOJM66Yjj5PQ9DhnC8LywZ7VH+tCtUbJtOJ8wr7gGnhynKD+jimXFJeY2RhXL9xarVpxdDLyh3EcHyf4ud/SSMkfqNry/A8):

![](../images/hwitw-globe.png)

Next, configure the application to show the climate variables of interest. Here, I show the extremes of the average daily temperature where I live in Juneau, AK. The cold temperatures from the 1950s and 1960s haven't been seen in decades, and now the hottest temperatures are constant throughout the summer.

![](../images/hwitw-jnu-temp.png)

### Kubernetes deployment

Let's examine the Kubernetes deployment. There are three major working in concert. Two CronJobs are configured, one to download data from Copernicus, and the other to transform that data into weekly summaries in a compact HDF format.

![](../images/hwitw-arch.png)

Each of these CronJobs spawns a series of Pods that run the containers with the `cdstool` and `tiletool` processes. Once the data has been output by tiletool, it can be accessed on the web. Web client sessions are handled by the Ingress, and dispatched to the HWITW Service, which in turn is implemented by a Pod running the `nginx` web server with the hwitw frontend HTML and Javascript code.

We can inspect these components more carefully in Kubernetes using the `kubectl` commandline utility.

:::{.column-page-right}

```bash
$ kubectl get services
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/hwitw   ClusterIP   10.109.0.227   <none>        5000/TCP   407d
```

That service is deployed in a pod running the `hwitw` container as a web service. In the pod listing below, you can also see that there are pods that have completed running for periodic cronjobs for `cdstool`, which downloads data from the Copernicus Data Service, and `tiletool`, which processes that data into weekly summaries and reorganizes it by time region in a large HDF5 file.

```bash
$ kubectl get pods
NAME                                   READY   STATUS      RESTARTS   AGE
pod/hwitw-67ccd577-rltrp               1/1     Running     0          160d
pod/hwitw-cdstool-28478538--1-99m2n    0/1     Completed   0          34d
pod/hwitw-cdstool-28508778--1-l5twt    0/1     Completed   0          13d
pod/hwitw-cdstool-28518858--1-xzpkn    0/1     Completed   0          6d10h
pod/hwitw-tiletool-28478543--1-28d2n   0/1     Completed   0          34d
pod/hwitw-tiletool-28508783--1-ldzdm   0/1     Completed   0          13d
pod/hwitw-tiletool-28518863--1-js8mj   0/1     Completed   0          6d10h
```

The CronJobs show the details of the schedule for these episodic processes, and the container implementation that was used for each run.

```bash
$ kubectl get cronjobs
NAME                           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/hwitw-cdstool    18 18 * * 5   False     0        6d10h           170d
cronjob.batch/hwitw-tiletool   23 18 * * 5   False     0        6d10h           170d

$ kubectl get jobs -o wide
NAME                      COMPLETIONS   DURATION   AGE     CONTAINERS          IMAGES
hwitw-cdstool-28478538    1/1           13s        34d     hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6
hwitw-cdstool-28508778    1/1           9s         13d     hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6
hwitw-cdstool-28518858    1/1           10s        6d11h   hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6
hwitw-tiletool-28478543   1/1           13s        34d     hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6
hwitw-tiletool-28508783   1/1           11s        13d     hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6
hwitw-tiletool-28518863   1/1           6s         6d10h   hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6
```
:::

Together, these Kubernetes resources, and a few others including an Ingress Controller to glue the application together, constitue a complete data access, data processing, and web visualization service for scientific data. The entire application can be deployed with a single command.

## Resources

- [Docker compose tutorial](https://www.baeldung.com/ops/docker-compose)
- [Kubernetes documentation](https://kubernetes.io/docs/concepts/)
- [Digital Ocean introduction to kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)